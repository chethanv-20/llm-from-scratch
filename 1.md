# Understanding the Core of AI and LLMs

With all the hype around AI and ChatGPT, many developers are eager to learn the core concepts behind these technologies. This guide explores the fundamental aspects you need to understand to become a better AI developer.

## Steps Involved in Training an AI Model

### 1. Pre-Training

#### Step 1: Download and Pre-process Data
Large Language Models (LLMs) are trained on massive datasets collected from the internet. For example, the [FineWeb v1 dataset](https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1) contains 15 trillion tokens and requires 44TB of disk space for pretraining.

**Data Processing Pipeline:**
- URL filtering → Text extraction
- Language filtering → Gopher filtering
- MinHash deduplication → C4 filtering
- Custom filtering → PII (Personally Identifiable Information) removal

Initially, we have a large amount of raw text. This text is converted into bits (zeros and ones). By combining 8 bits, we form a byte, which can be used as a unique pattern or identifier for similarity detection.

#### Step 2: Tokenization
Tokenization is the process of converting raw text into a sequence of symbols called tokens. One common method is Byte-Pair Encoding (BPE), where frequently occurring byte sequences are merged into unique tokens.

**Example:**
- ~5,000 Unicode characters ≈ 1,300 GPT tokens (out of 100,277 possible tokens)

Tokenization allows the model to efficiently process and understand text. You can explore tokenization further at [tiktokenizer.vercel.app](https://tiktokenizer.vercel.app).


and then we pass to nn and predict the 
next token - ie predict the probablity of next token

#### Step 3: Neural Network Training

After tokenization, the next step is to train a neural network on the tokenized data. We provide the model with a sequence of tokens (the length of this sequence, or "window size," is a parameter we choose). The neural network learns to predict the next token in the sequence, effectively modeling the probability distribution of possible next tokens.

This process enables the model to generate coherent and contextually relevant text.

**Fine-Tuning:**
In addition to pre-training, the model can be fine-tuned on specific datasets to specialize its behavior or improve performance on particular tasks. Fine-tuning involves continuing the training process with new, often smaller, datasets that are relevant to the desired application.